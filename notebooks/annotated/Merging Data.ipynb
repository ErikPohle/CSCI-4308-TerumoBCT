{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook tries to\n",
    "\n",
    "(1) reduce pandas frames' sizes\n",
    "\n",
    "(2) merge different frames\n",
    "\n",
    "(3) finally output a single Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas dataframes we are working with are very large, to make them easier to work with we will do a variety of tricks to shrink their memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mem_usage function checks the memory usage of a pandas object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reduce_mem_usage function tries to reduce a pandas object size by using \n",
    "\n",
    "(1) dropping empty columns \n",
    "\n",
    "(2) converting integer data type \n",
    "\n",
    "(3) converting float64 yo float32, and the final memory usage is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, make_sparse=False):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of dataframe is :\",start_mem_usg,\" MB\")\n",
    "    # First drop empty columns\n",
    "    df.dropna(axis=1,how='all', inplace=True)\n",
    "    for col in df.columns:\n",
    "        # Print current column type\n",
    "        print(\"******************************\")\n",
    "        print(\"Column: \",col)\n",
    "        print(\"dtype before: \",df[col].dtype)\n",
    "        if str(df[col].dtype) in [\"object\", \"string\"]:\n",
    "            print(df[col].nunique(), len(df[col]), (df[col].nunique() / len(df[col])))\n",
    "            if (df[col].nunique() / len(df[col])) < 0.5:\n",
    "                df.loc[:,col] = df[col].astype('category')\n",
    "        elif str(df[col].dtype).lower() == \"int64\": \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            try:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "            except ValueError:\n",
    "                df[col] = df[col].astype(\"Int64\")\n",
    "                # Make float datatypes 32 bit\n",
    "        elif str(df[col].dtype) == \"float64\":\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "        if (len(df[col].dropna()) / len(df[col]) < 0.25) and make_sparse:\n",
    "            df[col] = pd.arrays.SparseArray(df[col], dtype = df[col].dtype)\n",
    "        # Print new column type\n",
    "        print(\"dtype after: \",df[col].dtype)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100 * mem_usg / start_mem_usg,\"% of the initial size\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Data\n",
    "Some previous data exploration, checking for nans, num unique values, etc. has been omitted for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code blocks try to reduce the DataFrame size of the data and save the reducced data into the hard disk. Note that code blocks are quite similar, and they do almost the same things except that the data frames are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a613f7ead50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load all the raw data queried from the db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfact_blood_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/fact_blood_product.feather\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfact_donation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/fact_donation.feather\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfact_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/fact_exception.feather\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfact_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/fact_run.feather\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mstored\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \"\"\"\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# Load all the raw data queried from the db\n",
    "fact_blood_product = pd.read_feather(\"../data/raw/fact_blood_product.feather\")\n",
    "fact_donation = pd.read_feather(\"../data/raw/fact_donation.feather\")\n",
    "fact_exception = pd.read_feather(\"../data/raw/fact_exception.feather\")\n",
    "fact_run = pd.read_feather(\"../data/raw/fact_run.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of right now we don't know if some columns that share a name represent the same information.\n",
    "So for now going to rename the columns to keep the information seperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_blood_product.rename(columns={\n",
    "    \"number_of_units_processed\":\"number_of_blood_product_units_processed\"}, inplace=True)\n",
    "fact_donation.rename(columns={\n",
    "    \"number_of_units_processed\":\"number_of_donation_units_processed\",\n",
    "    \"number_of_duplicated_units\":\"number_of_duplicated_donation_units\",\n",
    "    \"number_of_skipped_barcodes\":\"number_of_skipped_donation_barcodes\",\n",
    "    \"number_of_alarms\":\"number_of_donation_alarms\",}, inplace=True)\n",
    "fact_exception.rename(columns={\n",
    "    \"number_of_units_processed\":\"number_of_donation_units_processed\",\n",
    "    \"number_of_duplicated_units\":\"number_of_duplicated_exception_units\",\n",
    "    \"number_of_skipped_barcodes\":\"number_of_skipped_exception_barcodes\",\n",
    "    \"number_of_alarms\":\"number_of_exception_alarms\",}, inplace=True)\n",
    "fact_run.rename(columns={\n",
    "    \"number_of_barcodes_skipped\":\"number_of_skipped_run_barcodes\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fact_blood_product & dim_blood_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns to best possible dtypes in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_blood_product = fact_blood_product.convert_dtypes()\n",
    "fact_blood_product.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_blood_product has a column with all NaNs get rid of it\n",
    "fact_blood_product.dropna(axis=1,how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets replace the dim_blood_product_id with the actuall blood products, this is essentially just a string replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_blood_product = pd.read_feather(\"../data/raw/dim_blood_product.feather\")\n",
    "blood_products = dim_blood_product.set_index('dim_blood_product_id')['blood_product_type'].to_dict()\n",
    "fact_blood_product[\"blood_product\"] = fact_blood_product[\"dim_blood_product_id\"].replace(blood_products)\n",
    "fact_blood_product.drop(columns=['dim_blood_product_id'], inplace = True)\n",
    "del dim_blood_product\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the first several rows in the fact_blood_product frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_blood_product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets clean up the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_blood_product = reduce_mem_usage(fact_blood_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_blood_product.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_blood_product.to_feather(\"../data/interim/fact_blood_product.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fact_donation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_donation = fact_donation.convert_dtypes()\n",
    "fact_donation.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_donation = reduce_mem_usage(fact_donation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_donation.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_donation.to_feather(\"../data/interim/fact_donation.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fact_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_exception = fact_exception.convert_dtypes()\n",
    "fact_exception.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_exception = reduce_mem_usage(fact_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_exception.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_exception.to_feather(\"../data/interim/fact_exception.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fact_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_run = fact_run.convert_dtypes()\n",
    "fact_run.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_run = reduce_mem_usage(fact_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_run.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_run.to_feather(\"../data/interim/fact_run.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging fact data\n",
    "\n",
    "### This section merges all available data into a single data frame by outer joining, and final output is a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_fact_data_1 = pd.merge(fact_blood_product, fact_donation, how='outer',\n",
    "                    left_on = ['dim_run_date', 'dim_device_id', 'dim_donation_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id'], \n",
    "                    right_on = ['dim_run_date', 'dim_device_id', 'dim_donation_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id']) \n",
    "\n",
    "del fact_blood_product\n",
    "del fact_donation\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_fact_data_2 = pd.merge(fact_exception, fact_run, how='outer',\n",
    "                    left_on = ['dim_run_date', 'dim_device_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id'], \n",
    "                    right_on = ['dim_run_date', 'dim_device_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id']) \n",
    "\n",
    "del fact_exception\n",
    "del fact_run\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = pd.merge(interim_fact_data_1, interim_fact_data_2, how='outer',\n",
    "                    left_on = ['dim_run_date', 'dim_device_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id'], \n",
    "                    right_on = ['dim_run_date', 'dim_device_id', 'dim_run_id', 'dim_facility_id', 'dim_configuration_id', 'dim_operator_id']) \n",
    "\n",
    "del interim_fact_data_1\n",
    "del interim_fact_data_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.convert_dtypes()\n",
    "fact_data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = reduce_mem_usage(fact_data)\n",
    "fact_data.to_feather(\"../data/interim/fact_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Kernal\n",
    "Do this to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = pd.read_feather(\"../data/interim/fact_data.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_configuration = pd.read_feather(\"../data/raw/dim_configuration.feather\")\n",
    "dim_configuration[\"configuration_status\"] = dim_configuration[\"configuration_status\"].astype(\"category\")\n",
    "dim_configuration.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_configuration, how='outer',\n",
    "                    left_on = ['dim_configuration_id'], \n",
    "                    right_on = ['dim_configuration_id']) \n",
    "fact_data.drop(columns=['dim_configuration_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_device = pd.read_feather(\"../data/raw/dim_device.feather\")\n",
    "# drop device_serial_number, device_name, and device_type_name(which is only 'REVEOS') I don't imagine they contain anything useful.\n",
    "dim_device.drop(columns=['device_serial_number', 'device_name', 'device_type_name'], inplace = True)\n",
    "dim_device[\"device_software_version\"] = dim_device[\"device_software_version\"].astype(\"category\")\n",
    "dim_device[\"device_language_name\"] = dim_device[\"device_language_name\"].astype(\"category\")\n",
    "dim_device.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_device, how='outer',\n",
    "                    left_on = ['dim_device_id'], \n",
    "                    right_on = ['dim_device_id']) \n",
    "fact_data.drop(columns=['dim_device_id'], inplace = True)\n",
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_donation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_donation = pd.read_feather(\"../data/raw/dim_donation.feather\")\n",
    "dim_donation[\"donation_status\"] = dim_donation[\"donation_status\"].astype(\"category\")\n",
    "#dim_donation[\"bucket_number\"] = dim_donation[\"bucket_number\"].astype(\"category\")\n",
    "dim_donation[\"unit_number_lifetime_status\"] = dim_donation[\"unit_number_lifetime_status\"].astype(\"category\")\n",
    "dim_donation[\"welding_status\"] = dim_donation[\"welding_status\"].astype(\"category\")\n",
    "dim_donation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_donation, how='outer',\n",
    "                    left_on = ['dim_donation_id'], \n",
    "                    right_on = ['dim_donation_id']) \n",
    "fact_data.drop(columns=['dim_donation_id'], inplace = True)\n",
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_exception = pd.read_feather(\"../data/raw/dim_exception.feather\")\n",
    "#dim_exception[\"bucket_number\"] = dim_exception[\"bucket_number\"].astype(\"category\")\n",
    "dim_exception[\"exception_type\"] = dim_exception[\"exception_type\"].astype(\"category\")\n",
    "dim_exception[\"exception_state\"] = dim_exception[\"exception_state\"].astype(\"category\")\n",
    "dim_exception.drop(columns=['run_data_message_entry_id'], inplace = True)\n",
    "dim_exception.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_exception, how='outer',\n",
    "                    left_on = ['dim_exception_id', 'bucket_number', 'run_datetime'], \n",
    "                    right_on = ['dim_exception_id', 'bucket_number', 'run_datetime']) \n",
    "fact_data.drop(columns=['dim_exception_id'], inplace = True)\n",
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_facility\n",
    "I am omitting this for now, facility names are almost unique and I don't see them being more helpful than the dim_faciliy_id. Also not sure what the facility time_zone will add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_operator = pd.read_feather(\"../data/raw/dim_operator.feather\")\n",
    "dim_operator.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_operator, how='outer',\n",
    "                    left_on = ['dim_operator_id', 'operator_id'], \n",
    "                    right_on = ['dim_operator_id', 'operator_id']) \n",
    "fact_data.drop(columns=['dim_operator_id'], inplace = True)\n",
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_run = pd.read_feather(\"../data/raw/dim_run.feather\")\n",
    "# dim_run has a column with all NaNs get rid of it\n",
    "dim_run.dropna(axis=1,how='all', inplace=True)\n",
    "dim_run.drop(columns=['file_name'], inplace = True)\n",
    "dim_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = fact_data.merge(dim_run, how='outer',\n",
    "                    left_on = ['dim_run_id', 'operator_id'], \n",
    "                    right_on = ['dim_run_id', 'operator_id']) \n",
    "fact_data.drop(columns=['dim_run_id'], inplace = True)\n",
    "fact_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some remaining datatypes and general cleanup of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data[\"bucket_number\"] = fact_data[\"bucket_number\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data = reduce_mem_usage(fact_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_data.to_feather(\"../data/interim/fact_data.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_custom_data_01 = pd.read_feather(\"../data/raw/dim_custom_data_01.feather\")\n",
    "dim_custom_data_02 = pd.read_feather(\"../data/raw/dim_custom_data_02.feather\")\n",
    "dim_custom_data_03 = pd.read_feather(\"../data/raw/dim_custom_data_03.feather\")\n",
    "dim_custom_data_04 = pd.read_feather(\"../data/raw/dim_custom_data_04.feather\")\n",
    "dim_custom_flag = pd.read_feather(\"../data/raw/dim_custom_flag.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_custom_data_01.iloc[:,2:] = dim_custom_data_01.iloc[:,2:].apply(pd.to_numeric, errors='raise')\n",
    "dim_custom_data_01 = dim_custom_data_01.convert_dtypes()\n",
    "dim_custom_data_01 = reduce_mem_usage(dim_custom_data_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_custom_data_02.iloc[:,2:] = dim_custom_data_02.iloc[:,2:].apply(pd.to_numeric, errors='raise')\n",
    "dim_custom_data_02 = dim_custom_data_02.convert_dtypes()\n",
    "dim_custom_data_02 = reduce_mem_usage(dim_custom_data_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_custom_data_03.iloc[:,2:] = dim_custom_data_03.iloc[:,2:].apply(pd.to_numeric, errors='raise')\n",
    "dim_custom_data_03 = dim_custom_data_03.convert_dtypes()\n",
    "dim_custom_data_03 = reduce_mem_usage(dim_custom_data_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_custom_data_04.iloc[:,2:] = dim_custom_data_04.iloc[:,2:].apply(pd.to_numeric, errors='raise')\n",
    "dim_custom_data_04 = dim_custom_data_04.convert_dtypes()\n",
    "dim_custom_data_04 = reduce_mem_usage(dim_custom_data_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_custom_flag.iloc[:,2:] = dim_custom_flag.iloc[:,2:].apply(pd.to_numeric, errors='raise')\n",
    "dim_custom_flag = dim_custom_flag.convert_dtypes()\n",
    "dim_custom_flag = reduce_mem_usage(dim_custom_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = reduce(\n",
    "                lambda x, y: pd.merge(\n",
    "                    x, \n",
    "                    y,\n",
    "                    how='outer',\n",
    "                    left_on = ['run_data_donation_id', 'dim_custom_data_id'], \n",
    "                    right_on = ['run_data_donation_id', 'dim_custom_data_id']), \n",
    "                    [dim_custom_data_01, dim_custom_data_02, dim_custom_data_03, dim_custom_data_04, dim_custom_flag]\n",
    "                )\n",
    "custom_data.drop(columns=['run_data_donation_id'], inplace = True)\n",
    "custom_data = reduce_mem_usage(custom_data)\n",
    "custom_data.to_feather(\"../data/interim/custom_data.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets merge *all* the data. We need to make the dataframes as small as possible because they will be huge in memory when pandas tries to merge them. To that end we're going to make everything a sparse category data type. We will go back and fix them after the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = pd.read_feather(\"../data/interim/custom_data.feather\")\n",
    "fact_data = pd.read_feather(\"../data/interim/fact_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in custom_data.columns[1:]:\n",
    "    custom_data[col] = custom_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in fact_data.columns:\n",
    "    if col != \"dim_custom_data_id\":\n",
    "        fact_data[col] = fact_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.merge(fact_data, custom_data, how='outer',\n",
    "                    left_on = ['dim_custom_data_id'], \n",
    "                    right_on = ['dim_custom_data_id']) \n",
    "final_data.drop(columns=['dim_custom_data_id'], inplace = True)\n",
    "fact_data = None\n",
    "cusomtom_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_feather(\"../data/interim/fact_and_custom_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"product_volume\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"product_volume\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(final_data[\"product_volume\"].astype(float), final_data[\"product_volume\"].astype(\"Int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "63314271 / final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"run_date\"] = pd.to_datetime(final_data[\"dim_run_date\"], infer_datetime_format=True)\n",
    "final_data.drop(columns=['dim_run_date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"run_date\"] = pd.arrays.SparseArray(final_data[\"run_date\"], dtype = final_data[\"run_date\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
